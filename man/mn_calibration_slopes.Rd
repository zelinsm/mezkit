% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mn_calibration_slopes.R
\name{mn_calibration_slopes}
\alias{mn_calibration_slopes}
\title{Calibration Slopes for Multinomial Models}
\usage{
mn_calibration_slopes(model, data, group_name = "Model")
}
\arguments{
\item{model}{A fitted multinomial model (e.g., created with \code{nnet::multinom()}).}

\item{data}{A data frame containing the variables used to fit the model.}

\item{group_name}{A character string labeling the model (used in output table).}
}
\value{
A tibble with one row per outcome class, and columns for:
\describe{
\item{model}{The name/label of the model (group_name).}
\item{outcome}{The class label for which calibration is assessed.}
\item{intercept}{The intercept of the calibration regression.}
\item{slope}{The slope of the calibration regression, measuring calibration.}
}
}
\description{
Estimates calibration slopes and intercepts for each outcome class predicted by a multinomial model.
This method evaluates how well the predicted probabilities for each class align with the observed class labels,
using simple linear regression: \eqn{(Observed ~ Predicted)} for each class separately.
}
\details{
A slope close to 1.0 indicates good calibration (predicted probabilities match observed frequencies).
A slope > 1 suggests overconfidence (model probabilities are too extreme), while a slope < 1 suggests underconfidence.

This function is intended for use with multiclass classification models where the predicted outcome
is a factor variable with more than two levels. It calculates, for each class, a binary indicator
of whether the observed label equals the class, and regresses this indicator on the predicted probability
for that class. This is a simple and interpretable approach to assessing calibration for multiclass models.
}
\examples{
library(nnet)
model <- multinom(Species ~ Sepal.Length + Sepal.Width, data = iris)
calibration_slopes(model, iris, group_name = "iris_model")

}
